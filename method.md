## Motivation
通过对1,013个硬件设计45个特征的综合分析，我们发现当在一个corner上训练的模型在不同corner上评估时，性能急剧下降，经常产生负R²分数。具体而言，在Fast corner数据上训练的模型在Fast corner数据上测试时达到R² = 0.994，但在Slow corner数据上测试时R² = -0.742。这个负R²表明模型的表现比简单地预测均值还要差，暗示着corner特异性功耗机制之间存在根本性不兼容。

这种跨corner预测失败源于三个主要挑战:

1. 控制功耗消耗的物理机制在不同corner间存在实质性差异，需要不同的特征重要性层次结构。在Fast corner中，频率相关特征(如xlen、decoders)对功耗有显著影响，因为动态功耗与开关频率直接相关。然而，在Slow corner中，这些特征的影响大大减少，而面积相关特征(如缓存容量、执行单元数量)变得更加重要，因为漏电功耗主要由晶体管总数决定。

2. 特征交互的复杂性显著变化，动态功耗表现出复杂的非线性关系，而漏电功耗通常遵循更简单的模式。在Fast corner中，不同特征之间存在强烈的交互效应—例如，处理器频率的增加会同时影响所有执行单元的功耗，创建了一个复杂的相互依赖网络。相比之下，Slow corner的漏电功耗主要是各个组件漏电的线性累加，特征间的交互相对简单。

3. 传统的ensemble方法采用静态权重方案，无法考虑预测不确定性和上下文相关的可靠性。当不同corner模型对同一设计给出不同预测时，我们需要智能地确定哪个预测更可靠，这需要考虑模型的不确定性、预测的一致性以及设计的具体特征。

虽然Transformer在自然语言处理和计算机视觉领域取得了巨大成功，但其在硬件建模中的应用仍处于探索阶段。现有应用主要将标准Transformer架构直接应用于硬件数据，而没有考虑硬件设计的特定特征和物理约束。这种直接应用忽略了硬件系统的结构化特性和不同组件间的物理关系。标准Transformer架构假设所有位置和特征具有相同的重要性和交互模式，这与硬件系统中不同组件具有不同物理作用和相互影响的现实不符。我们的工作填补了这一空白，通过设计专门针对硬件功耗建模和corner特异性的Transformer架构。

以往的建模尝试通常采用简单的模型分离策略，为每个corner训练独立模型，而不考虑潜在的物理原理或最优整合方法。这些方法存在几个局限性：它们缺乏corner特异性架构的系统设计原则，采用忽略基于物理学先验的静态特征权重，并使用简单的平均策略进行模型整合。

## Contribution

1. 基于corner特异性Transformer架构

2. 智能ensemble整合具有不确定性感知权重分配。

## Method

Stage 1: Corner特异性Transformer训练阶段 专注于为每个corner设计和训练专门的Transformer模型。这一阶段的核心思想是让网络架构直接反映不同corner的物理特性差异。我们不再试图用一个模型适应所有corner，而是为每个corner量身定制最适合其物理机制的网络结构。
    1.1 传统Transformer问题: 所有应用场景使用相同的注意力配置，忽略了domain-specific特性,我们的解决方案: 基于corner物理特性定制注意力机制
    Fast Corner特异性设计:
    更多注意力头: 捕获频率相关特征间的复杂交互
    更深的网络: 建模高动态功耗的非线性关系
    较低dropout: 保持动态特征的敏感性

    Slow Corner特异性设计:
    较少注意力头: 专注于静态特征的局部关系
    相对简单网络: 避免对静态功耗的过复杂建模
    较高dropout: 防止对漏电特征的过拟合

    Typical Corner特异性设计:
    平衡配置: 兼顾动态和静态特征
    标准深度: 适应混合功耗特性

    我们对标准Transformer注意力机制进行了corner特异性扩展，引入物理驱动的注意力偏置：

    $$\text{Attention}{corner}(Q,K,V) = \text{softmax}\left(\frac{QK^T + B{corner}}{\sqrt{d_k}}\right)V$$

    其中B{corner}是可学习的corner特异性偏置矩阵，其设计基于深刻的物理动机：

    Fast Corner注意力设计：
    B_fast = +0.1 × 全1矩阵 (正偏置)
    物理动机：动态功耗具有全局传播特性
    - 频率变化影响整个芯片的开关活动
    - 需要捕获远程特征间的复杂依赖关系
    - 正偏置鼓励注意力分布更加分散
    设计参数：nhead=8, 多头并行捕获不同交互模式

    Slow Corner注意力设计：
    B_slow = -0.1 × 全1矩阵 (负偏置)  
    物理动机：漏电功耗主要是局部累加效应
    - 每个组件的漏电主要由自身特性决定
    - 远程依赖关系相对较弱
    - 负偏置促使注意力更加集中
    设计参数：nhead=4, 较少注意力头专注局部特征

    Typical Corner注意力设计：
    B_typical = 0 × 全1矩阵 (零偏置)
    物理动机：混合功耗需要平衡的注意力模式
    - 既要考虑全局动态影响，又要关注局部静态特性
    - 标准注意力机制适合平衡需求
    设计参数：nhead=8, 平衡的多头配置
    
    这种设计的优雅之处在于它通过最小的架构修改实现了最大的物理对应性，让网络的计算模式与物理传播机制直接对齐。



    1.2  多尺度特征融合适应不同corner的复杂度需求

        Fast Corner
        卷积核配置：[1×1, 3×3, 5×5, 7×7]
        物理动机：动态功耗的全局传播特性
        - 时钟信号变化影响所有同步组件
        - 数据路径拥塞影响整个流水线
        - 需要多尺度捕获不同范围的动态影响
        传播范围：局部→模块→簇→全局的层次化影响

        Slow Corner多尺度设计：
        卷积核配置：[1×1, 3×3]
        物理动机：漏电功耗的局部累加特性
        - 每个组件漏电主要由自身特性决定
        - 与邻近组件有一定关联，远程依赖较弱
        - 专注局部和近邻特征聚合
        传播范围：单元级→模块级的有限影响

        Typical Corner
        卷积核配置：[1×1, 3×3, 5×5]
        物理动机：混合功耗的平衡传播特性
        - 需要同时捕获动态和静态影响
        - 标准多尺度配置适应多样化需求
        - 平衡局部精确性和全局感知能力
        传播范围：局部→模块→区域的渐进影响



Stage 2: 智能Ensemble整合阶段 传统ensemble通常使用固定权重或简单的性能加权，我们的方法考虑了预测不确定性、应用场景需求和设计上下文，实现了更加智能和自适应的融合。

静态权重的局限性
问题：传统方法使用固定权重 [1/3, 1/3, 1/3] 或基于验证性能的静态权重
缺陷：忽略了不同预测场景下模型可靠性的动态变化
影响：无法适应不同设计的特征分布和预测难度差异

传统ensemble方法给每个模型固定权重，我们的方法根据每个模型的预测可靠性动态调整权重。
**动态权重分配**：根据每个模型的预测不确定性动态调整权重

$$w_i = \frac{1/\sigma_i^2}{\sum_{j=1}^{3} 1/\sigma_j^2}$$

其中 $\sigma_i^2$ 是第 $i$ 个corner模型的预测不确定性。

**核心逻辑**：不确定性越小 → 权重越大

核心逻辑：不确定性越小 → 权重越大


### 不确定性是如何量化的？

每个corner模型都配备一个专门的不确定性估计头：

```python
uncertainty_head = Sequential(
    Linear(512, 256),    # 压缩特征
    GELU(),             # 激活函数
    Linear(256, 64),     # 进一步压缩
    GELU(),
    Linear(64, 1),       # 输出不确定性
    Softplus()          # 确保输出为正值
)
```

### 多源不确定性融合

我们考虑三种不确定性来源：

1. **预测不确定性**：模型对当前预测的置信度
   $$\sigma_{pred}^2 = \text{UncertaintyHead}(\text{特征})$$

2. **特征权重不确定性**：特征权重的变化程度
   $$\sigma_{weight}^2 = \text{Var}(\text{特征权重})$$

3. **模型一致性不确定性**：三个corner模型预测的分歧程度
   $$\sigma_{consistency}^2 = \text{Var}(y_{fast}, y_{slow}, y_{typical})$$

**综合不确定性**：
$$\sigma_{total}^2 = 0.6 \times \sigma_{pred}^2 + 0.3 \times \sigma_{weight}^2 + 0.1 \times \sigma_{consistency}^2$$

---
### 3. 贝叶斯权重分配公式

**步骤1：计算逆不确定性权重**
$$w_{raw,i} = \frac{1}{\sigma_{total,i}^2 + \epsilon}$$

其中 $\epsilon = 10^{-8}$ 防止除零。

**步骤2：归一化权重**
$$w_{normalized,i} = \frac{w_{raw,i}}{\sum_{j=1}^{3} w_{raw,j}}$$

**物理意义**：
- 不确定性小的模型 → 高权重
- 不确定性大的模型 → 低权重





# 更详细的emsemble权重
**核心思想**：让预测更可靠的模型获得更高的权重。

#### 3.1.1 不确定性量化方法

每个corner模型通过专门的不确定性估计头输出预测置信度：

```python
uncertainty_head = nn.Sequential(
    nn.Linear(d_model, d_model // 2),
    nn.GELU(),
    nn.Dropout(0.1),
    nn.Linear(d_model // 2, 1),
    nn.Softplus()  # 确保输出为正值
)
```

**多源不确定性综合**：
$$\sigma_{total,i}^2(\mathbf{x}) = 0.6 \cdot \sigma_{pred,i}^2(\mathbf{x}) + 0.3 \cdot \sigma_{weight,i}^2(\mathbf{x}) + 0.1 \cdot \sigma_{consistency}^2(\mathbf{x})$$

其中：
- $\sigma_{pred,i}^2$：预测不确定性
- $\sigma_{weight,i}^2$：特征权重变化不确定性
- $\sigma_{consistency}^2$：模型间一致性不确定性

#### 3.1.2 贝叶斯权重分配公式

**基础权重计算**：
$$w_{uncertainty,i}(\mathbf{x}) = \frac{1}{\sigma_{total,i}^2(\mathbf{x}) + \epsilon}$$

**归一化权重**：
$$w_{normalized,i}(\mathbf{x}) = \frac{w_{uncertainty,i}(\mathbf{x})}{\sum_{j=1}^{3} w_{uncertainty,j}(\mathbf{x})}$$

### 3.2 多层次权重调制策略

**Level 1: 场景感知基础权重**
```
DSE优化场景：     [Fast: 0.2, Slow: 0.2, Typical: 0.6]
设计验证场景：     [Fast: 0.4, Slow: 0.4, Typical: 0.2]
功耗预算场景：     [Fast: 0.1, Slow: 0.3, Typical: 0.6]
性能优化场景：     [Fast: 0.5, Slow: 0.3, Typical: 0.2]
```

**Level 2: 不确定性调制**
$$w_{modulated} = \alpha \times w_{base} + (1-\alpha) \times w_{uncertainty}$$

**Level 3: 上下文自适应优化**
根据设计复杂度、功耗敏感度等特征进行最终权重调整。