import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score, mean_absolute_percentage_error
from sklearn.preprocessing import StandardScaler
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
import torch.nn.functional as F
import os
import warnings
warnings.filterwarnings('ignore')

# --- Manual Control & Device Setup ---
FORCE_CPU = False

if not FORCE_CPU and torch.cuda.is_available():
    device = torch.device("cuda")
    print("GPU is available. Using CUDA for training.")
else:
    device = torch.device("cpu")
    if FORCE_CPU:
        print("FORCE_CPU is set to True. Using CPU for training.")
    else:
        print("ğŸ’» GPU not available. Using CPU for training.")


# --- Transformer Model Definition ---
class TransformerPredictor(nn.Module):
    def __init__(self, num_features, d_model=256, nhead=8, num_encoder_layers=8, dim_feedforward=256, dropout=0.1):
        super(TransformerPredictor, self).__init__()
        self.d_model = d_model
        # Input embedding layer to project features to d_model
        self.input_embedding = nn.Linear(num_features, d_model)
        
        # åŸºäºæ•°æ®åˆ†æå’Œç‰©ç†åŸç†çš„cornerç‰¹æ€§
        self.corner_physics = {
            'Fast': {  # High-V, Low-T: é«˜åŠ¨æ€åŠŸè€—ï¼Œä½æ¼ç”µ
                'dynamic_boost': 2.0,
                'frequency_boost': 1.8,
                'control_boost': 1.5,
                'static_penalty': 0.8,
                'focus_features': ['xlen', 'decoders', 'lanes', 'with-mul', 'with-div', 'pipeline_complexity']
            },
            'Slow': {  # Low-V, High-T: ä½åŠ¨æ€åŠŸè€—ï¼Œé«˜æ¼ç”µ
                'static_boost': 2.2,
                'leakage_boost': 2.0,
                'area_boost': 1.8,
                'dynamic_penalty': 0.7,
                'focus_features': ['fetch-l1-capacity', 'lsu-l1-capacity', 'cache_total_complexity', 'execution_complexity']
            },
            'Typical': {  # Nominal-V, Room-T: å¹³è¡¡çŠ¶æ€
                'balanced_boost': 1.5,
                'control_boost': 1.4,
                'cache_boost': 1.3,
                'general_factor': 1.0,
                'focus_features': ['branch_pred_complexity', 'with-gshare', 'with-btb', 'pipeline_complexity']
            }
        }
        
        # å¯å­¦ä¹ çš„ç‰©ç†æƒé‡ç½‘ç»œ
        self.physics_weight_net = nn.Sequential(
            nn.Linear(num_features, num_features * 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(num_features * 2, num_features),
            nn.Sigmoid()  # è¾“å‡º0-1çš„æƒé‡
        )
        
        # Cornerç‰¹å¼‚æ€§çš„å›ºå®šæƒé‡ (åŸºäºç‰©ç†åˆ†æ)
        self.register_buffer('corner_bias', torch.ones(num_features))
        self._initialize_corner_bias()
    
    def _initialize_corner_bias(self):
        """åŸºäºcornerç‰©ç†ç‰¹æ€§åˆå§‹åŒ–åç½®"""
        corner_config = self.corner_physics[self.corner_type]
        
        if self.corner_type == 'Fast':
            self.corner_bias *= corner_config['dynamic_boost']
        elif self.corner_type == 'Slow':
            self.corner_bias *= corner_config['static_boost']
        else:  # Typical
            self.corner_bias *= corner_config['balanced_boost']
    
    def forward(self, features):
        # å­¦ä¹ çš„è‡ªé€‚åº”æƒé‡
        learned_weights = self.physics_weight_net(features)
        
        # ç»“åˆç‰©ç†åç½®å’Œå­¦ä¹ æƒé‡
        corner_bias_expanded = self.corner_bias.unsqueeze(0).expand_as(features)
        combined_weights = learned_weights * corner_bias_expanded
        
        # åº”ç”¨æƒé‡
        weighted_features = features * combined_weights
        
        return weighted_features, combined_weights


class UnifiedCornerSpecificTransformer(nn.Module):
    """
    ç»Ÿä¸€ç»´åº¦çš„Cornerç‰¹å¼‚æ€§Transformeræ¨¡å‹
    æ‰€æœ‰cornerä½¿ç”¨ç›¸åŒçš„d_modelä»¥é¿å…ç»´åº¦ä¸åŒ¹é…
    """
    def __init__(self, num_features, corner_type='Fast', d_model=128, nhead=8, 
                 num_encoder_layers=4, dim_feedforward=512, dropout=0.1):
        super().__init__()
        self.corner_type = corner_type
        self.d_model = d_model  # ç»Ÿä¸€çš„d_model
        self.num_features = num_features
        
        # Stage 1é›†æˆ: ç‰©ç†æ„ŸçŸ¥ç‰¹å¾æƒé‡
        self.feature_weighting = PhysicsAwareFeatureWeighting(num_features, corner_type)
        
        # Cornerç‰¹å¼‚æ€§é…ç½® - ä¿æŒd_modelç»Ÿä¸€ï¼Œè°ƒæ•´å…¶ä»–å‚æ•°
        corner_configs = {
            'Fast': {
                'nhead': 8, 'num_layers': 4, 'dropout': 0.08,
                'dim_feedforward': 512,
                'description': 'High dynamic power, frequency sensitive'
            },
            'Slow': {
                'nhead': 4, 'num_layers': 3, 'dropout': 0.12,  # ä½¿ç”¨4ä¸ªheadç¡®ä¿æ•´é™¤
                'dim_feedforward': 384,
                'description': 'High leakage power, area sensitive'
            },
            'Typical': {
                'nhead': 8, 'num_layers': 4, 'dropout': 0.10,
                'dim_feedforward': 512,
                'description': 'Balanced power characteristics'
            }
        }
        
        config = corner_configs[corner_type]
        
        print(f"Initializing {corner_type} Corner Model: {config['description']}")
        print(f"   d_model={d_model}, nhead={config['nhead']}, layers={config['num_layers']}")
        
        # è¾“å…¥åµŒå…¥å±‚ - ç»Ÿä¸€è¾“å‡ºç»´åº¦
        self.input_embedding = nn.Sequential(
            nn.Linear(num_features, d_model * 2),
            nn.LayerNorm(d_model * 2),
            nn.GELU(),
            nn.Dropout(config['dropout']),
            nn.Linear(d_model * 2, d_model)
        )
        
        # ä½ç½®ç¼–ç  - ç»Ÿä¸€ç»´åº¦
        self.positional_encoding = nn.Parameter(
            torch.randn(1, num_features, d_model) * 0.02
        )
        
        # Cornerç‰¹å¼‚æ€§Transformerç¼–ç å™¨
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=config['nhead'],
            dim_feedforward=config['dim_feedforward'],
            dropout=config['dropout'],
            activation='gelu',
            batch_first=True,
            norm_first=True
        )
        
        self.transformer_encoder = nn.TransformerEncoder(
            encoder_layer, 
            num_layers=config['num_layers']
        )
        
    def forward(self, src):
        embedded_src = self.input_embedding(src)  # -> (batch_size, d_model)
        transformer_input = embedded_src.unsqueeze(1)  # -> (batch_size, 1, d_model)
        transformer_output = self.transformer_encoder(transformer_input) # -> (batch_size, 1, d_model)
        transformer_output = transformer_output.squeeze(1) # -> (batch_size, d_model)
        output = self.output_layer(transformer_output) # -> (batch_size, 1)
        return output


# --- Training and Evaluation Function ---
def run_transformer_pipeline(X_train, y_train, X_test, y_test, num_features, model_path, epochs=1000, batch_size=8, lr=0.00005, patience=100):
    """
    Handles the entire process of scaling, training, and evaluating the Transformer model.
    Includes learning rate scheduler and early stopping.
    """
    # Scale features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # Create PyTorch Datasets and DataLoaders
    train_dataset = TensorDataset(torch.tensor(X_train_scaled, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))
    test_dataset = TensorDataset(torch.tensor(X_test_scaled, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32))
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    
    # Initialize model, loss, and optimizer
    model = TransformerPredictor(num_features=num_features).to(device)
    criterion = nn.MSELoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.05)
    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)


    # Early stopping variables
    best_loss = float('inf')
    patience_counter = 0

    # Training loop
    for epoch in range(epochs):
        model.train()
        total_loss = 0
        for data, targets in train_loader:
            data, targets = data.to(device), targets.to(device)
            
            # åªåœ¨éæœ€åä¸€å±‚æ·»åŠ æ¿€æ´»å’Œå½’ä¸€åŒ–
            if i < len(config['layers']) - 2:
                layers.append(nn.LayerNorm(config['layers'][i+1]))
                if config['activation'] == 'gelu':
                    layers.append(nn.GELU())
                else:
                    layers.append(nn.ReLU())
                layers.append(nn.Dropout(dropout * (0.5 ** i)))
        
        # Check if validation loss improves
        # scheduler.step(total_loss / len(train_loader))
        scheduler.step()

        if (epoch + 1) % 10 == 0:
            print(f'Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}')
        
        # Early stopping
        if total_loss < best_loss:
            best_loss = total_loss
            patience_counter = 0
            # Save the best model
            torch.save({
                'model_state_dict': model.state_dict(),
                'scaler': scaler
            }, model_path)
        else:
            patience_counter += 1

        if patience_counter >= patience:
            print(f"Early stopping at epoch {epoch + 1}")
            break


# =============================================================================
# STAGE 2: ENSEMBLE INTEGRATION
# =============================================================================

class AdvancedEnsembleIntegrator(nn.Module):
    """
    Stage 2: é«˜çº§Ensembleæ•´åˆå™¨
    æ™ºèƒ½èåˆä¸‰ä¸ªcorneræ¨¡å‹çš„é¢„æµ‹
    """
    def __init__(self, d_model=64):
        super().__init__()
        self.d_model = d_model
        
        # åŸºäºæ•°æ®åˆ†æçš„å…ˆéªŒæƒé‡
        self.integration_methods = {
            'weighted_avg': torch.tensor([0.2, 0.2, 0.6]),  # Fast, Slow, Typical
            'worst_case': torch.tensor([0.4, 0.4, 0.2]),     # æ›´å…³æ³¨æç«¯æƒ…å†µ
            'best_case': torch.tensor([0.3, 0.3, 0.4]),      # æ›´å…³æ³¨å…¸å‹æƒ…å†µ
            'rms': torch.tensor([0.33, 0.33, 0.34]),         # ç­‰æƒé‡RMS
            'conservative': torch.tensor([0.1, 0.3, 0.6])     # ä¿å®ˆä¼°è®¡
        }
        
        # åŠ¨æ€æƒé‡é¢„æµ‹ç½‘ç»œ
        self.dynamic_weight_net = nn.Sequential(
            nn.Linear(6, d_model),  # 3ä¸ªé¢„æµ‹å€¼ + 3ä¸ªä¸ç¡®å®šæ€§å€¼
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(d_model, d_model // 2),
            nn.GELU(),
            nn.Linear(d_model // 2, 3),
            nn.Softmax(dim=1)
        )
        
        # ä¸ç¡®å®šæ€§åŠ æƒç½‘ç»œ
        self.uncertainty_weight_net = nn.Sequential(
            nn.Linear(3, d_model // 2),  # 3ä¸ªä¸ç¡®å®šæ€§å€¼
            nn.GELU(),
            nn.Linear(d_model // 2, 3),
            nn.Softmax(dim=1)
        )
        
        # ä¸Šä¸‹æ–‡æ„ŸçŸ¥æƒé‡è°ƒæ•´
        self.context_adjustment = nn.Sequential(
            nn.Linear(3, d_model // 4),  # 3ä¸ªé¢„æµ‹å€¼
            nn.GELU(),
            nn.Linear(d_model // 4, 3),
            nn.Tanh()  # è¾“å‡ºè°ƒæ•´å› å­
        )
    
    def forward(self, predictions, uncertainties=None, method='adaptive'):
        """
        predictions: [batch_size, 3] - Fast, Slow, Typicalé¢„æµ‹
        uncertainties: [batch_size, 3] - å¯¹åº”çš„ä¸ç¡®å®šæ€§ (å¯é€‰)
        method: æ•´åˆæ–¹æ³•
        """
        batch_size = predictions.size(0)
        device = predictions.device
        
        if method == 'adaptive' and uncertainties is not None:
            # è‡ªé€‚åº”æƒé‡æ•´åˆ
            
            # åŸºäºé¢„æµ‹å€¼å’Œä¸ç¡®å®šæ€§çš„åŠ¨æ€æƒé‡
            input_features = torch.cat([predictions, uncertainties], dim=1)
            dynamic_weights = self.dynamic_weight_net(input_features)
            
            # åŸºäºä¸ç¡®å®šæ€§çš„æƒé‡ (ä¸ç¡®å®šæ€§è¶Šå°æƒé‡è¶Šå¤§)
            uncertainty_weights = self.uncertainty_weight_net(uncertainties)
            inv_uncertainty = 1.0 / (uncertainties + 1e-8)
            inv_uncertainty = inv_uncertainty / inv_uncertainty.sum(dim=1, keepdim=True)
            
            # ä¸Šä¸‹æ–‡è°ƒæ•´
            context_adj = self.context_adjustment(predictions)
            
            # ç»„åˆæƒé‡
            combined_weights = 0.4 * dynamic_weights + 0.4 * inv_uncertainty + 0.2 * uncertainty_weights
            combined_weights = combined_weights * (1 + 0.1 * context_adj)
            
            # é‡æ–°å½’ä¸€åŒ–
            final_weights = combined_weights / combined_weights.sum(dim=1, keepdim=True)
            
        elif method in self.integration_methods:
            # ä½¿ç”¨é¢„å®šä¹‰çš„æƒé‡
            prior_weights = self.integration_methods[method].to(device)
            final_weights = prior_weights.unsqueeze(0).repeat(batch_size, 1)
            
        else:
            # é»˜è®¤ç­‰æƒé‡
            final_weights = torch.ones(batch_size, 3, device=device) / 3
        
        # æ‰§è¡ŒåŠ æƒèåˆ
        integrated_prediction = (predictions * final_weights).sum(dim=1, keepdim=True)
        
        return integrated_prediction, final_weights


# =============================================================================
# THREE-STAGE TRAINING PIPELINE
# =============================================================================

class ThreeStageCornerTrainer:
    """
    Cornerç‰¹å¼‚æ€§è®­ç»ƒæµç¨‹
    """
    def __init__(self, df_chipfinish, features):
        self.df_chipfinish = df_chipfinish
        self.features = features
        self.power_cols = ['Power1', 'Power2', 'Power3']
        self.corner_names = ['Fast', 'Slow', 'Typical']
        
        # å­˜å‚¨è®­ç»ƒç»“æœ
        self.corner_models = {}
        self.corner_scalers = {}
        self.stage1_results = {}
        
        print(f"Initializing Three-Stage Corner Training")
        print(f"Dataset: {len(df_chipfinish)} designs, {len(features)} features")
        print(f"Corners: {self.corner_names}")
    
    def stage1_train_corner_models(self, epochs=300, batch_size=32, lr=0.001):
        """
        Stage 1: è®­ç»ƒCornerç‰¹å¼‚æ€§æ¨¡å‹
        """
        print(f"\n" + "="*60)
        print(f"STAGE 1: CORNER-SPECIFIC MODEL TRAINING")
        print(f"="*60)
        
        for power_col, corner_name in zip(self.power_cols, self.corner_names):
            print(f"\nTraining {corner_name} Corner Model")
            print(f"-" * 40)
            
            # å‡†å¤‡æ•°æ®
            X = self.df_chipfinish[self.features].select_dtypes(include=[np.number]).values
            y = self.df_chipfinish[power_col].values * 1e-6  # è½¬æ¢ä¸ºmW
            
            # æ•°æ®åˆ†å‰²
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42
            )
            
            # ç‰¹å¾ç¼©æ”¾ (ä½¿ç”¨QuantileTransformeræ›´robust)
            scaler = QuantileTransformer(output_distribution='normal', random_state=42)
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            # ç›®æ ‡å˜æ¢ (logå˜æ¢)
            y_train_log = np.log1p(y_train)
            y_test_log = np.log1p(y_test)
            
            print(f"   Data: {X_train.shape[0]} train, {X_test.shape[0]} test")
            print(f"   Power range: {y_train.min():.6f} - {y_train.max():.6f} mW")
            
            # åˆ›å»ºæ•°æ®åŠ è½½å™¨
            train_dataset = TensorDataset(
                torch.FloatTensor(X_train_scaled),
                torch.FloatTensor(y_train_log)
            )
            test_dataset = TensorDataset(
                torch.FloatTensor(X_test_scaled),
                torch.FloatTensor(y_test_log)
            )
            
            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
            
            # åˆ›å»ºç»Ÿä¸€ç»´åº¦çš„cornerç‰¹å¼‚æ€§æ¨¡å‹
            model = UnifiedCornerSpecificTransformer(
                num_features=X.shape[1],
                corner_type=corner_name,
                d_model=128  # ç»Ÿä¸€çš„d_model
            ).to(device)
            
            # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
            optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)
            scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
                optimizer, T_0=50, eta_min=lr*0.01
            )
            
            # ç»„åˆæŸå¤±å‡½æ•°
            def corner_specific_loss(pred, target, corner_type):
                huber_loss = F.huber_loss(pred.squeeze(), target, delta=0.5)
                mse_loss = F.mse_loss(pred.squeeze(), target)
                
                # Cornerç‰¹å¼‚æ€§æƒé‡
                if corner_type == 'Fast':
                    return 0.8 * huber_loss + 0.2 * mse_loss  # æ›´å…³æ³¨robustness
                elif corner_type == 'Slow':
                    return 0.6 * huber_loss + 0.4 * mse_loss  # å¹³è¡¡
                else:  # Typical
                    return 0.7 * huber_loss + 0.3 * mse_loss
            
            # è®­ç»ƒå¾ªç¯
            best_loss = float('inf')
            patience = 0
            patience_limit = 25
            
            print(f"   Starting training for {epochs} epochs...")
            
            for epoch in range(epochs):
                model.train()
                total_loss = 0
                num_batches = 0
                
                for batch_x, batch_y in train_loader:
                    batch_x, batch_y = batch_x.to(device), batch_y.to(device)
                    
                    optimizer.zero_grad()
                    pred = model(batch_x)
                    loss = corner_specific_loss(pred, batch_y, corner_name)
                    
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                    optimizer.step()
                    scheduler.step()
                    
                    total_loss += loss.item()
                    num_batches += 1
                
                avg_loss = total_loss / num_batches
                
                # éªŒè¯å’Œæ—©åœ
                if epoch % 20 == 0 or epoch == epochs - 1:
                    model.eval()
                    val_preds = []
                    val_targets = []
                    
                    with torch.no_grad():
                        for batch_x, batch_y in test_loader:
                            batch_x = batch_x.to(device)
                            pred = model(batch_x)
                            val_preds.extend(pred.cpu().numpy().flatten())
                            val_targets.extend(batch_y.numpy())
                    
                    # è½¬æ¢å›åŸå§‹å°ºåº¦
                    val_preds = np.expm1(np.array(val_preds))
                    val_targets = np.expm1(np.array(val_targets))
                    
                    val_mape = mean_absolute_percentage_error(val_targets, val_preds)
                    val_r2 = r2_score(val_targets, val_preds)
                    
                    print(f"   Epoch [{epoch+1:3d}/{epochs}]: Loss={avg_loss:.6f}, "
                          f"Val MAPE={val_mape:.2%}, Val RÂ²={val_r2:.4f}")
                
                # æ—©åœæ£€æŸ¥
                if avg_loss < best_loss:
                    best_loss = avg_loss
                    patience = 0
                    
                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    model_path = f'{corner_name.lower()}_corner_model_unified.pth'
                    torch.save({
                        'model_state_dict': model.state_dict(),
                        'epoch': epoch,
                        'loss': best_loss,
                        'corner_type': corner_name
                    }, model_path)
                    
                    scaler_path = f'{corner_name.lower()}_corner_scaler_unified.pkl'
                    joblib.dump(scaler, scaler_path)
                else:
                    patience += 1
                
                if patience >= patience_limit:
                    print(f"   â¹ï¸ Early stopping at epoch {epoch+1}")
                    break
            
            # åŠ è½½æœ€ä½³æ¨¡å‹å¹¶è¿›è¡Œæœ€ç»ˆè¯„ä¼°
            checkpoint = torch.load(f'{corner_name.lower()}_corner_model_unified.pth')
            model.load_state_dict(checkpoint['model_state_dict'])
            
            # æœ€ç»ˆè¯„ä¼°
            model.eval()
            final_preds = []
            final_targets = []
            final_uncertainties = []
            
            with torch.no_grad():
                for batch_x, batch_y in test_loader:
                    batch_x = batch_x.to(device)
                    pred, uncertainty = model(batch_x, return_uncertainty=True)
                    
                    final_preds.extend(pred.cpu().numpy().flatten())
                    final_uncertainties.extend(uncertainty.cpu().numpy().flatten())
                    final_targets.extend(batch_y.numpy())
            
            final_preds = np.expm1(np.array(final_preds))
            final_targets = np.expm1(np.array(final_targets))
            
            mae = mean_absolute_error(final_targets, final_preds)
            mape = mean_absolute_percentage_error(final_targets, final_preds)
            r2 = r2_score(final_targets, final_preds)
            
            print(f"   Final Results: MAE={mae:.6f}, MAPE={mape:.2%}, RÂ²={r2:.4f}")
            
            # å­˜å‚¨ç»“æœ
            self.corner_models[corner_name] = model
            self.corner_scalers[corner_name] = scaler
            self.stage1_results[corner_name] = {
                'mae': mae, 'mape': mape, 'r2': r2,
                'model_path': f'{corner_name.lower()}_corner_model_unified.pth',
                'scaler_path': f'{corner_name.lower()}_corner_scaler_unified.pkl'
            }
        
        return self.stage1_results
    
    # Print results
    print(f"MAE: {mae:.2f}")
    print(f"MAPE: {mape:.2%}")
    print(f"R^2 Score: {r2:.4f}")
    
    print(f"Model and scaler saved to '{model_path}'\n")


# --- Main Script Logic ---
df = pd.read_csv('preprocessed_data.csv')

def main_three_stage_corner_training():
    """
    ä¸»å‡½æ•°ï¼šcornerç‰¹å¼‚æ€§è®­ç»ƒ
    """
    print(f"Loading data and starting three-stage corner training...")
    
    # ä½¿ç”¨æ‚¨ç°æœ‰çš„æ•°æ®é¢„å¤„ç†é€»è¾‘
    df = pd.read_csv('power_with_vex_parameters.csv')
    df = preprocess_hardware_data(df)
    
    # è¿‡æ»¤å®Œæ•´è®¾è®¡
    phase_counts = df.groupby('Design Name')['Backend Phase'].nunique()
    complete_phases_count = phase_counts.max()
    complete_designs = phase_counts[phase_counts == complete_phases_count].index
    df_filtered = df[df['Design Name'].isin(complete_designs)].copy()
    
    # èšç„¦chipfinishé˜¶æ®µ
    df_chipfinish = df_filtered[df_filtered['Backend Phase'] == 'chipfinish'].copy()
    
    # è·å–ç‰¹å¾
    feature_start_idx = df_chipfinish.columns.get_loc('xlen')
    all_features = list(df_chipfinish.columns[feature_start_idx:])
    features_base = [f for f in all_features if not f.startswith('Power')]
    
    print(f"Data loaded and preprocessed:")
    print(f"   {len(df_chipfinish)} designs")
    print(f"   {len(features_base)} features")
    print(f"   âš¡ 3 power corners (Fast, Slow, Typical)")
    
    # åˆ›å»ºå¹¶è¿è¡Œè®­ç»ƒå™¨
    trainer = ThreeStageCornerTrainer(df_chipfinish, features_base)
    stage1_results, stage2_results, stage3_results = trainer.run_complete_pipeline()
    
    return trainer, stage1_results, stage2_results, stage3_results


# ä½¿ç”¨æ‚¨ç°æœ‰çš„é¢„å¤„ç†å‡½æ•°
def preprocess_hardware_data(df):
    """
    Enhanced preprocessing for integrated power prediction
    """
    print("--- Enhanced Data Preprocessing for Corner-Specific Models ---")
    
    # Convert boolean columns
    for col in df.columns:
        if df[col].dtype == 'bool':
            df[col] = df[col].astype(int)
    
    # Enhanced feature engineering for power prediction
    if 'fetch-l1-sets' in df.columns and 'fetch-l1-ways' in df.columns:
        df['fetch-l1-capacity'] = df['fetch-l1-sets'] * df['fetch-l1-ways']
        df['fetch-l1-complexity'] = df['fetch-l1-sets'] + df['fetch-l1-ways']
    
    if 'lsu-l1-sets' in df.columns and 'lsu-l1-ways' in df.columns:
        df['lsu-l1-capacity'] = df['lsu-l1-sets'] * df['lsu-l1-ways']
        df['lsu-l1-complexity'] = df['lsu-l1-sets'] + df['lsu-l1-ways']
    
    # Power-related complexity indicators
    df['branch_pred_complexity'] = (
        df.get('with-gshare', 0) + df.get('with-btb', 0) + df.get('with-ras', 0)
    )
    
    df['execution_complexity'] = (
        df.get('with-mul', 0) + df.get('with-div', 0) + df.get('with-rvc', 0)
    )
    
    # Cache total complexity
    df['cache_total_complexity'] = (
        df.get('fetch-l1-capacity', 0) + df.get('lsu-l1-capacity', 0)
    )
    
    # Pipeline complexity
    df['pipeline_complexity'] = (
        df.get('decoders', 0) * df.get('lanes', 0) * df.get('dispatcher-at', 1)
    )
    
    print(f"Added {7} engineered features for corner-specific power prediction")
    return df


# # --- Scenario 2: Predicting with floorplan power ---
print("--- Scenario 2 (Filtered Data): Predicting with floorplan power as a feature ---")
df_fp_pivot = df_filtered[df_filtered['Backend Phase'].isin(['chipfinish', 'floorplan'])].pivot_table(
    index='Design Name', columns='Backend Phase', values='Power3'
).reset_index()
df_params = df_filtered.drop_duplicates(subset='Design Name')[['Design Name'] + features_base]
df_merged = pd.merge(df_fp_pivot, df_params, on='Design Name').dropna()

features_with_floorplan = ['floorplan'] + features_base
target_chipfinish = 'chipfinish'
X2 = df_merged[features_with_floorplan].values
y2 = df_merged[target_chipfinish].values * 1e-6
X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=42)
run_transformer_pipeline(X2_train, y2_train, X2_test, y2_test, X2.shape[1], 'transformer_model_with_floorplan.pth')
